# ãƒ­ãƒ¼ã‚«ãƒ«æ¨è«–ãƒŸãƒ‹å®Ÿé¨“ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—

> **ç›®çš„**: CPUâ€‘only ç’°å¢ƒã§ã‚‚å‹•ãè¶…è»½é‡ LLM ã‚’è‡ªåŠ›ã§ãƒ“ãƒ«ãƒ‰ã—ã€Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æŠ•ã’ã¦ã¿ã‚‹ã€‚ä¸€é€£ã®æµã‚Œã‚’ä½“é¨“ã™ã‚‹ã“ã¨ã§ã€ã‚¯ãƒ©ã‚¦ãƒ‰ API ã¨ã®é•ã„ã‚„é‡å­åŒ–ã®æ¦‚å¿µã‚’ç†è§£ã™ã‚‹ã€‚

---

## 0. äº‹å‰æ¡ä»¶

|        | æ¨å¥¨                                     | å‚™è€ƒ                |
| ------ | -------------------------------------- | ----------------- |
| OS     | WindowsÂ 10/11, macOSÂ 11+, UbuntuÂ 22.04 | *WSL2 å¯*          |
| CPU    | SSE4.2 (Intel / AMD) or AppleÂ Silicon  | GPU ä¸è¦            |
| Python | 3.9Â â€“Â 3.12                             | `venv` or `conda` |
| Git    | 2.x                                    |                   |

> âš  **ãƒ¡ãƒ¢ãƒªç›®å®‰**: Tiny ãƒ¢ãƒ‡ãƒ« (1.1â€¯B) ã® 4â€‘bit é‡å­åŒ–ã§ â‰’Â 0.8â€¯GBâ€‘RAMã€‚

---

## 1. ãƒªãƒã‚¸ãƒˆãƒªæ§‹æˆ

```text
local-llm-demo/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md   â† ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ setup.sh    â† llama.cpp ãƒ“ãƒ«ãƒ‰ï¼‹ãƒ¢ãƒ‡ãƒ«DL (bash)
â”œâ”€â”€ local_chat.py â† Python ãƒ©ãƒƒãƒ‘ (llamaâ€‘cppâ€‘python)
â””â”€â”€ models/
    â””â”€â”€ tinyllama-1.1b-chat.gguf (è‡ªå‹•DL)
```

---

## 2. ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ (TL;DR)

```bash
# 1ï¸âƒ£ ã‚¯ãƒ­ãƒ¼ãƒ³
$ git clone https://github.com/<yourname>/local-llm-demo.git
$ cd local-llm-demo

# 2ï¸âƒ£ ä¸€ç™ºã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ— (10ã€œ15 åˆ†)
$ bash setup.sh

# 3ï¸âƒ£ ãƒãƒ£ãƒƒãƒˆé–‹å§‹
$ python local_chat.py "æ—¥æœ¬ã®é¦–éƒ½ã¯ï¼Ÿ"
```

---

## 3. è©³ç´°ã‚¹ãƒ†ãƒƒãƒ—

### 3.1 ä»®æƒ³ç’°å¢ƒä½œæˆ

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install --upgrade pip
```

### 3.2 llama.cpp ã®ãƒ“ãƒ«ãƒ‰

`setup.sh` å†…éƒ¨ã§ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚

```bash
# llama.cpp ã‚½ãƒ¼ã‚¹å–å¾—
if [ ! -d "llama.cpp" ]; then
  git clone https://github.com/ggerganov/llama.cpp.git
fi
cd llama.cpp
make -j$(nproc) LLAMA_NO_AVX2=1  # Apple Silicon ã¯ LLAMA_METAL=1
cd ..
```

### 3.3 é‡å­åŒ–æ¸ˆã¿ TinyLlama ãƒ¢ãƒ‡ãƒ«ã®å–å¾—

```bash
MODEL_URL="https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v0.4-GGUF/resolve/main/TinyLlama-1.1B-Chat-v0.4.Q4_K_M.gguf"
mkdir -p models && wget -O models/tinyllama-1.1b-chat.gguf "$MODEL_URL"
```

> ğŸ” *ãªãœé‡å­åŒ–?* â€” é‡ã¿ã‚’ 16/8 bit â‡’ 4 bit ã¸åœ§ç¸®ã—ã€ãƒ¡ãƒ¢ãƒªå¸¯åŸŸã¨ã‚µã‚¤ã‚ºã‚’å‰Šæ¸›ã€‚

### 3.4 Python ä¾å­˜ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
pip install llama-cpp-python==0.2.* "python-dotenv<1" rich typer
```

---

## 4. ã‚¹ã‚¯ãƒªãƒ—ãƒˆè§£èª¬

### 4.1 `local_chat.py`

```python
#!/usr/bin/env python
"""æœ€å° LLAMA cpp ãƒ©ãƒƒãƒ‘ã€‚
$ python local_chat.py "ã“ã‚“ã«ã¡ã¯" --n-predict 128
"""
import argparse, pathlib, os
from llama_cpp import Llama, LlamaCompletion

ROOT = pathlib.Path(__file__).resolve().parent
MODEL = ROOT / "models" / "tinyllama-1.1b-chat.gguf"

parser = argparse.ArgumentParser()
parser.add_argument("prompt", help="ãƒ¦ãƒ¼ã‚¶ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ")
parser.add_argument("--n-predict", type=int, default=64, help="ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°")
args = parser.parse_args()

llm = Llama(model_path=str(MODEL), n_ctx=2048, n_threads=os.cpu_count())
completion: LlamaCompletion = llm(
    args.prompt,
    max_tokens=args.n_predict,
    temperature=0.7,
    top_p=0.9,
    stop=["</s>", "USER:"],
)
print("\n=== ç”Ÿæˆçµæœ ===\n", completion["choices"][0]["text"].strip())
```

> **ãƒã‚¤ãƒ³ãƒˆ**
>
> * `n_ctx`: æ–‡è„ˆé•· (token) â€” é•·ã„ã»ã©ãƒ¡ãƒ¢ãƒªå¢—
> * `n_threads`: ç‰©ç†ã‚³ã‚¢æ•° â‰’ æœ€é€Ÿ

---

## 5. ã‚ˆãã‚ã‚‹è³ªå• (FAQ)

| è³ªå•                        | å›ç­”                                                              |
| ------------------------- | --------------------------------------------------------------- |
| *M1/M2 Mac ã§ Metal ã‚’ä½¿ã„ãŸã„* | `make LLAMA_METAL=1` ã§ãƒ“ãƒ«ãƒ‰ & `local_chat.py` ã« `n_gpu_layers` æŒ‡å®š |
| *AVX2 ç„¡ã— PC ã§å¤±æ•—ã™ã‚‹*        | `make LLAMA_NO_AVX2=1` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»˜ä¸                                 |
| *ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã™ãã‚‹*               | ã•ã‚‰ã« 3â€‘bit (Q3\_K\_M) ã‚„ distil ç‰ˆã‚’ä½¿ã†                              |
| *è¤‡æ•°ã‚¿ãƒ¼ãƒ³ãƒãƒ£ãƒƒãƒˆã—ãŸã„*            | prompt ã‚’ `system + history + user` ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ã—ã¦ãƒ«ãƒ¼ãƒ—å®Ÿè£…               |

---

## 6. æ•™æå¿œç”¨ã‚¢ã‚¤ãƒ‡ã‚¢

1. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¯”è¼ƒ**: æ¸©åº¦ / Topâ€‘p ã‚’å¤‰ãˆã¦å‰µé€ æ€§ã‚’è¦³å¯Ÿ
2. **RAG å…¥é–€**: `chromadb` ã¸ embedding ã‚’ç™»éŒ²ã—ã€retrieval ã—ãŸæ–‡ã‚’ prompt å‰æ®µã«çµåˆ
3. **MAS å…¥é–€**: `crewai` ã§ Retrieverâ€‘Agent â†” Writerâ€‘Agent ã‚’å¯¾è©±ã•ã›ã‚‹

---

## 7. ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ãƒ»å¼•ç”¨

* ã‚³ãƒ¼ãƒ‰: MIT
* TinyLlamaâ€‘1.1Bâ€‘Chat: Apacheâ€‘2.0ï¼ˆccÂ TinyLlama Teamï¼‰

> å­¦è¡“ç›®çš„ã§ã®åˆ©ç”¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ãŒã€ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã£ã¦ãã ã•ã„ã€‚

---

## 8. å‚è€ƒãƒªãƒ³ã‚¯

* ggerganov/llama.cpp â€” [https://github.com/ggerganov/llama.cpp](https://github.com/ggerganov/llama.cpp)
* TinyLlamaâ€‘1.1Bâ€‘Chat GGUF â€” [https://huggingface.co/TinyLlama](https://huggingface.co/TinyLlama)
* llamaâ€‘cppâ€‘python â€” [https://github.com/abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
